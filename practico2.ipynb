{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Practico 2: Análisis y Curación\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definamos variables que serán de utilidad.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Por temas de bajo poder de computo vamos a usar solo un dataframe\n",
        "df = pd.read_csv(\"./sarcasm_v2/GEN-sarc-notsarc.csv\")\n",
        "# df = pd.read_csv(\"./sarcasm_v2/HYP-sarc-notsarc.csv\")\n",
        "# df = pd.read_csv(\"./sarcasm_v2/RQ-sarc-notsarc.csv\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_df = df[\"sarc\" == df[\"class\"]][\"text\"]\n",
        "notsarc_df = df[\"notsarc\" == df[\"class\"]][\"text\"]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones útiles\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flatten_list = lambda nested_list: [\n",
        "    el for sublist in nested_list for el in sublist]\n",
        "\n",
        "def compare_freq(most_common_freq, cmp_freq,\n",
        "                 mc_label, cmp_label,\n",
        "                 mc_color, cmp_color):\n",
        "    \"\"\"\n",
        "    This function compares the frequency of the most common tokens\n",
        "    of `most_common_freq` with the frequency they have in `cmp_freq`.\n",
        "    \"\"\"\n",
        "    most_common = most_common_freq.most_common(30)\n",
        "\n",
        "    most_common_words = [x for x, y in most_common]\n",
        "    most_common_freqs = [y/len(most_common_freq) for x, y in most_common]\n",
        "\n",
        "    cmp_freq_in_mc = [cmp_freq[x]/len(cmp_freq) for x, y in most_common]\n",
        "\n",
        "    most_common = sn.lineplot(\n",
        "        x=most_common_words,\n",
        "        y=cmp_freq_in_mc,\n",
        "        label=cmp_label,\n",
        "        sort=False,\n",
        "        color=cmp_color\n",
        "    )\n",
        "    not_sarc_gr = sn.lineplot(\n",
        "        x=most_common_words,\n",
        "        y=most_common_freqs,\n",
        "        label=mc_label,\n",
        "        sort=False,\n",
        "        color=mc_color\n",
        "    )\n",
        "    rot_lab_ns = most_common.set_xticklabels(\n",
        "        labels=most_common_words, rotation=90)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_token_texts = [nlp(snts.lower()) for snts in sarc_df]\n",
        "not_sarc_token_texts = [\n",
        "    nlp(snts.lower()) for snts in notsarc_df]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_token_list = flatten_list(sarc_token_texts)\n",
        "not_sarc_token_list = flatten_list(not_sarc_token_texts)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lematización\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_lemm_tokens = [word.lemma_ for word in sarc_token_list]\n",
        "not_sarc_lemm_tokens = [word.lemma_ for word in not_sarc_token_list]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_lemm_freq = Counter(sarc_lemm_tokens)\n",
        "not_sarc_lemm_freq = Counter(not_sarc_lemm_tokens)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_freq(\n",
        "    sarc_lemm_freq, not_sarc_lemm_freq,\n",
        "    \"sarcasm\", \"not sarcasm\",\n",
        "    \"blue\", \"red\",\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_freq(\n",
        "    not_sarc_lemm_freq, sarc_lemm_freq,\n",
        "    \"not sarcasm\", \"sarcasm\",\n",
        "    \"red\", \"blue\",\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy no tiene una herramienta para stemming por lo cual no es posible hacer\n",
        "ese transformación sobre los tokens.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heatmap\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df0 = pd.read_csv(\"./sarcasm_v2/GEN-sarc-notsarc.csv\")\n",
        "df1 = pd.read_csv(\"./sarcasm_v2/HYP-sarc-notsarc.csv\")\n",
        "df2 = pd.read_csv(\"./sarcasm_v2/RQ-sarc-notsarc.csv\")\n",
        "\n",
        "df_concat = pd.concat([df0, df1, df2], ignore_index=True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_df = df_concat[\"sarc\" == df_concat[\"class\"]]\n",
        "notsarc_df = df_concat[\"notsarc\" == df_concat[\"class\"]]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_tokens = []\n",
        "for doc in sarc_df['text']:\n",
        "    tokenized_doc = nlp(doc)\n",
        "    for t in tokenized_doc:\n",
        "        if t.is_stop == False and t.is_punct == False:\n",
        "            sarc_tokens.append(t.lower_)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notsarc_tokens = []\n",
        "for doc in notsarc_df['text']:\n",
        "    tokenized_doc = nlp(doc)\n",
        "    for t in tokenized_doc:\n",
        "        if t.is_stop == False and t.is_punct == False:\n",
        "            notsarc_tokens.append(t.lower_)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_token_freq = Counter(sarc_tokens)\n",
        "not_sarc_token_freq = Counter(notsarc_tokens)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sarc_token = pd.DataFrame.from_dict(sarc_token_freq, orient='index')\n",
        "df_sarc_token.columns = ['Frequencia Sarcasmo']\n",
        "df_sarc_token.index.name = 'Termino'\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_not_sarc = pd.DataFrame.from_dict(not_sarc_token_freq, orient='index')\n",
        "df_not_sarc.columns = ['Frequencia No Sarcasmo']\n",
        "df_not_sarc.index.name = 'Termino'\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BINS = pd.IntervalIndex.from_tuples([(1, 5), (6, 30), (31, 9999)],)\n",
        "df = dict(zip(BINS,[\"bajo\", \"medio\", \"alto\"]))\n",
        "\n",
        "freq_sarc = pd.cut(df_sarc_token['Frequencia Sarcasmo'], BINS).map(df)\n",
        "freq_not_sarc = pd.cut(df_not_sarc['Frequencia No Sarcasmo'], BINS).map(df)\n",
        "\n",
        "df_1 = pd.concat([df_sarc_token, freq_sarc, df_not_sarc, freq_not_sarc], axis=1)\n",
        "df_1.columns = [\n",
        "    'Frecuencia_Sarcasmo',\n",
        "    'Frecuencia_categorica_Sarcasmo',\n",
        "    'Frecuencia_NoSarc',\n",
        "    'Frecuencia_categorica_NoSarc'\n",
        "]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap = pd.crosstab(\n",
        "    df_1.Frecuencia_categorica_Sarcasmo,\n",
        "    df_1.Frecuencia_categorica_NoSarc,\n",
        "    normalize = False\n",
        ")\n",
        "sn.heatmap(heatmap, annot=True, fmt=\"d\", linewidths=.5, cmap=\"YlGnBu\")\n",
        "plt.title(\"Heatmap de frecuencias\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver en el heatmap que mientras mas frecuentes son las palabras de ambas\n",
        "categorias menos coinciden entre si.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-GRAMAS\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_docs_bigrams(texts):\n",
        "    \"\"\"Get all bigrams of the corpus\"\"\"\n",
        "    docs_bigrams = []\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        sent_bigrams = []\n",
        "        for sent in doc.sents:\n",
        "            sent_bigrams.append(\n",
        "                [[sent[ind], sent[ind + 1]] for ind in range(len(sent)-1)]\n",
        "            )\n",
        "        docs_bigrams.append(sent_bigrams)\n",
        "    return docs_bigrams\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = get_docs_bigrams(sarc_df)\n",
        "print(bigrams[0])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_docs_trigrams(texts):\n",
        "    \"\"\"Get all trigrams of the corpus\"\"\"\n",
        "    docs_trigrams = []\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        sent_trigrams = []\n",
        "        for sent in doc.sents:\n",
        "            sent_trigrams.append(\n",
        "                [\n",
        "                    [sent[ind], sent[ind + 1], sent[ind + 2]]\n",
        "                    for ind in range(len(sent)-2)\n",
        "                ]\n",
        "            )\n",
        "        docs_trigrams.append(sent_trigrams)\n",
        "    return docs_trigrams\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = get_docs_trigrams(sarc_df)\n",
        "print(trigrams[0])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy tiene una funcion para poder ver la relación que hay entre los diferentes\n",
        "tokens de un documento. Utilizando esta función podemos conseguir bigramas que\n",
        "aporten más contexto a la relación que tienen las dos palabras y también obviar\n",
        "bigramas que sean poco relevantes, por ejemplo la puntuación, o relaciones no\n",
        "definidas (que aparecen con tokens que no son palabras).\n",
        "\n",
        "Veamos un grafico de este tipo de dependencias en una oración.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "def display_dep(doc):\n",
        "    \"\"\"Display token dependencies in a document\"\"\"\n",
        "    displacy.render(\n",
        "        doc, style=\"dep\")\n",
        "\n",
        "display_dep(list(nlp(sarc_df[2609]).sents)[0])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXCLUDE_DEPS = [\"punct\", \"ccmp\", \"prep\", \"pobj\", \"X\", \"space\", \"\", \"ROOT\"]\n",
        "def get_doc_dep_bigrams(texts):\n",
        "    \"\"\"\n",
        "    Get all relevant bigrams of tokens that may not be next to each other\n",
        "    \"\"\"\n",
        "    docs_bigrams = []\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        sent_trigrams = []\n",
        "        for sent in doc.sents:\n",
        "            for token in sent:\n",
        "                if token.dep_ in EXCLUDE_DEPS:\n",
        "                    continue\n",
        "                sent_trigrams.append([token.text, token.head.text, token.dep_])\n",
        "            docs_bigrams.append(sent_trigrams)\n",
        "    return docs_bigrams\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_dep_bigrams = get_doc_dep_bigrams(sarc_df)\n",
        "print(doc_dep_bigrams[0])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf\n",
        "def get_doc_tf(text):\n",
        "    \"\"\"Get dict of token frequency of a document\"\"\"\n",
        "    doc = nlp(text)\n",
        "    doc_tf = {}\n",
        "\n",
        "    tokens = [\n",
        "        token for token in doc\n",
        "        if not (token.is_stop or token.is_punct or token.is_space)\n",
        "    ]\n",
        "    token_count = len(tokens)\n",
        "\n",
        "    for t in tokens:\n",
        "        doc_tf[t.lower_] = doc_tf.get(t.lower_, 0) + 1\n",
        "\n",
        "    for t in doc_tf:\n",
        "        doc_tf[t] = doc_tf[t] / token_count\n",
        "\n",
        "    return doc_tf\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# idf\n",
        "def get_docs_idf(texts):\n",
        "    \"\"\"\n",
        "    Get Inverse Document Frequency in all documents of a corpus.\n",
        "    \"\"\"\n",
        "    docs_idf = {}\n",
        "    doc_count = len(texts)\n",
        "\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # important to notice this is a set (not an array)\n",
        "        # and therefore tokens will only appear once\n",
        "        tokens = {\n",
        "            token for token in doc\n",
        "            if not (token.is_stop or token.is_punct or token.is_space)\n",
        "        }\n",
        "        token_count = len(tokens)\n",
        "\n",
        "        for t in tokens:\n",
        "            docs_idf[t.lower_] = docs_idf.get(t.lower_, 0) + 1\n",
        "\n",
        "    for token in docs_idf:\n",
        "        docs_idf[token] = math.log(doc_count/docs_idf[token])\n",
        "\n",
        "    return docs_idf\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf\n",
        "def get_doc_tfidf(doc_tf, docs_idf):\n",
        "    \"\"\"\n",
        "    Get TF-IDF of all tokens in a document using its TF and corpus IDF.\n",
        "    \"\"\"\n",
        "    doc_tfidf = {\n",
        "        k: doc_tf[k] * docs_idf[k]\n",
        "        for k in doc_tf\n",
        "    }\n",
        "\n",
        "    return doc_tfidf\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_above_avg_tokens(docs_tfidf):\n",
        "    \"\"\"\n",
        "    Filter tokens that have an above average TF-IDF sum.\n",
        "    \"\"\"\n",
        "    # get the tokens that have a tf-idf above average of each document\n",
        "    above_avg_tfidf = []\n",
        "\n",
        "    for tfidf in docs_tfidf:\n",
        "        for token in tfidf:\n",
        "            above_avg_tokens = {}\n",
        "            if tfidf[token] > np.average(list(tfidf.values())):\n",
        "                above_avg_tokens[token] = tfidf[token]\n",
        "        # if no token is above average we wont be taking it into account\n",
        "        if above_avg_tokens != {}:\n",
        "            above_avg_tfidf.append(above_avg_tokens)\n",
        "\n",
        "    return above_avg_tfidf\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sum_tfidf(above_avg_tfidf):\n",
        "    \"\"\"\n",
        "    Get sum of TF-IDFs of above average tokens in each document of the corpus\n",
        "    \"\"\"\n",
        "    tokens_tfidf_sum = {}\n",
        "\n",
        "    for doc in above_avg_tfidf:\n",
        "        for token in doc:\n",
        "            curr_token_value = tokens_tfidf_sum.get(token, 0)\n",
        "            tokens_tfidf_sum[token] = curr_token_value + doc[token]\n",
        "    # sort important tokens\n",
        "    sorted_tfidf_sum = sorted(\n",
        "        tokens_tfidf_sum.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_tfidf_sum\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lineplot_graph(tuples, label):\n",
        "    \"\"\"Graph a lineplot taking list of tuples (x, y)\"\"\"\n",
        "    x = [x for x, y in tuples]\n",
        "    y = [y for x, y in tuples]\n",
        "\n",
        "    graph = sn.lineplot(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        label=label,\n",
        "        sort=False,\n",
        "    )\n",
        "    x_labels_fix = graph.set_xticklabels(\n",
        "        labels=x, rotation=90)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_docs_idf = get_docs_idf(sarc_df)\n",
        "notsarc_docs_idf = get_docs_idf(notsarc_df)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_docs_tf = [get_doc_tf(doc) for doc in sarc_df]\n",
        "notsarc_docs_tf = [get_doc_tf(doc) for doc in notsarc_df]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_docs_tfidf = [\n",
        "    get_doc_tfidf(doc_tf, sarc_docs_idf)\n",
        "    for doc_tf in sarc_docs_tf\n",
        "]\n",
        "notsarc_docs_tfidf = [\n",
        "    get_doc_tfidf(doc_tf, notsarc_docs_idf)\n",
        "    for doc_tf in notsarc_docs_tf\n",
        "]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ab_avg_sarc_tokens = get_above_avg_tokens(sarc_docs_tfidf)\n",
        "ab_avg_notsarc_tokens = get_above_avg_tokens(notsarc_docs_tfidf)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_tfidf_sum = get_sum_tfidf(ab_avg_sarc_tokens)\n",
        "notsarc_tfidf_sum = get_sum_tfidf(ab_avg_notsarc_tokens)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_sarc = [x for x,y in sarc_tfidf_sum]\n",
        "y_sarc = [y for x,y in sarc_tfidf_sum]\n",
        "sarc_label = \"SARC TF-IDF\"\n",
        "\n",
        "x_notsarc = [x for x,y in notsarc_tfidf_sum]\n",
        "y_notsarc = [y for x,y in notsarc_tfidf_sum]\n",
        "notsarc_label = \"NOT SARC TF-IDF\"\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lineplot_graph(\n",
        "    sarc_tfidf_sum[:15], sarc_label,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lineplot_graph(\n",
        "    notsarc_tfidf_sum[:15], notsarc_label,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver en los graficos, la relación entre las palabras con TF-IDF más\n",
        "alto de Sarcasmo y No Sarcasmo es practicamente nula. Esto es consistente\n",
        "con los datos vistos en el heatmap anteriormente.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis de Entidades\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus_entities(texts):\n",
        "    \"\"\"Get dict of entities in the corpus separated by category\"\"\"\n",
        "    docs_entities = {}\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        for ent in doc.ents:\n",
        "            doc_ent_set = docs_entities.get(ent.label_, set())\n",
        "            doc_ent_set.add(ent.text)\n",
        "            docs_entities[ent.label_] = doc_ent_set\n",
        "    return docs_entities\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_entities = get_corpus_entities(sarc_df)\n",
        "list(corpus_entities.keys())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# entity tf\n",
        "def get_doc_entity_tf(text):\n",
        "    \"\"\"Get all entities Term Frequency in a document\"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    entities_count = 0\n",
        "    docs_entities = {}\n",
        "    doc_tf = {}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        docs_entities[ent.label_] = docs_entities.get(ent.label_, 0) + 1\n",
        "\n",
        "    for ent in docs_entities:\n",
        "        entities_count = entities_count + docs_entities.get(ent, 0)\n",
        "\n",
        "    for e in docs_entities:\n",
        "        doc_tf[e] = docs_entities[e] / entities_count\n",
        "\n",
        "    return doc_tf\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# entity idf\n",
        "def get_docs_entities_idf(texts):\n",
        "    \"\"\"Get all entities IDF in the corpus\"\"\"\n",
        "    docs_idf = {}\n",
        "    doc_count = len(texts)\n",
        "\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # important to notice this is a set (not an array)\n",
        "        # and therefore entitites will only appear once\n",
        "        docs_entities = {\n",
        "            ent.label_ for ent in doc.ents\n",
        "        }\n",
        "\n",
        "        entities_count = len(docs_entities)\n",
        "\n",
        "        for e in docs_entities:\n",
        "            docs_idf[e] = docs_idf.get(e, 0) + 1\n",
        "\n",
        "    for ent in docs_idf:\n",
        "        docs_idf[ent] = math.log(doc_count/docs_idf[ent])\n",
        "\n",
        "    return docs_idf\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_docs_entity_idf = get_docs_entities_idf(sarc_df)\n",
        "notsarc_docs_entity_idf = get_docs_entities_idf(notsarc_df)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_docs_entity_tf = [get_doc_entity_tf(doc) for doc in sarc_df]\n",
        "notsarc_docs_entity_tf = [get_doc_entity_tf(doc) for doc in notsarc_df]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_docs_entity_tfidf = [\n",
        "    get_doc_tfidf(doc_tf, sarc_docs_entity_idf)\n",
        "    for doc_tf in sarc_docs_entity_tf\n",
        "]\n",
        "notsarc_docs_entity_tfidf = [\n",
        "    get_doc_tfidf(doc_tf, notsarc_docs_entity_idf)\n",
        "    for doc_tf in notsarc_docs_entity_tf\n",
        "]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ab_avg_sarc_entities = get_above_avg_tokens(sarc_docs_entity_tfidf)\n",
        "ab_avg_notsarc_entities = get_above_avg_tokens(notsarc_docs_entity_tfidf)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarc_entity_tfidf_sum = get_sum_tfidf(ab_avg_sarc_entities)\n",
        "notsarc_entity_tfidf_sum = get_sum_tfidf(ab_avg_notsarc_entities)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_sarc_entity = [x for x,y in sarc_entity_tfidf_sum]\n",
        "y_sarc_entity = [y for x,y in sarc_entity_tfidf_sum]\n",
        "sarc_label = \"SARC ENTITY TF-IDF\"\n",
        "\n",
        "notsarc_label = \"NOT SARC ENTITY TF-IDF\"\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lineplot_graph(\n",
        "    sarc_entity_tfidf_sum[:15], sarc_label,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lineplot_graph(\n",
        "    notsarc_entity_tfidf_sum[:15], notsarc_label,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}